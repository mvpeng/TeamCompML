% Template file for an a0 landscape poster.
% Written by Graeme, 2001-03 based on Norman's original microlensing
% poster.
%
% See discussion and documentation at
% <http://www.astro.gla.ac.uk/users/norman/docs/posters/> 
%
% $Id: poster-template-landscape.tex,v 1.2 2002/12/03 11:25:46 norman Exp $


% Default mode is landscape, which is what we want, however dvips and
% a0poster do not quite do the right thing, so we end up with text in
% landscape style (wide and short) down a portrait page (narrow and
% long). Printing this onto the a0 printer chops the right hand edge.
% However, 'psnup' can save the day, reorienting the text so that the
% poster prints lengthways down an a0 portrait bounding box.
%
% 'psnup -w85cm -h119cm -f poster_from_dvips.ps poster_in_landscape.ps'

\documentclass[a0]{a0poster}
% You might find the 'draft' option to a0 poster useful if you have
% lots of graphics, because they can take some time to process and
% display. (\documentclass[a0,draft]{a0poster})
\input defs
\pagestyle{empty}
\setcounter{secnumdepth}{0}
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\QED}{~~\rule[-1pt]{8pt}{8pt}}\def\qed{\QED}

\renewcommand{\reals}{{\mbox{\bf R}}}

% The textpos package is necessary to position textblocks at arbitary 
% places on the page.
\usepackage[absolute]{textpos}

\usepackage{fleqn,psfrag,wrapfig,tikz,amsmath, framed, scrextend,subcaption}
\usepackage{mathtools,url}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\usepackage[papersize={38in,28in}]{geometry}

% Graphics to include graphics. Times is nice on posters, but you
% might want to switch it off and go for CMR fonts.
\usepackage{graphics}

\renewenvironment{leftbar}[1][\hsize]
{% 
\def\FrameCommand 
{%

    {\color{black}\vrule width 0pt}%
    \hspace{0pt}%must no space.
    \fboxsep=\FrameSep\colorbox{white}%
}%
\MakeFramed{\hsize#1\advance\hsize-\width\FrameRestore}%
}
{\endMakeFramed}

% we are running pdflatex, so convert .eps files to .pdf
%\usepackage[pdftex]{graphicx}
%\usepackage{epstopdf}

% These colours are tried and tested for titles and headers. Don't
% over use color!
\usepackage{color}
\definecolor{Red}{rgb}{0.9,0.0,0.1}

\definecolor{bluegray}{rgb}{0.15,0.20,0.40}
\definecolor{bluegraylight}{rgb}{0.35,0.40,0.60}
\definecolor{gray}{rgb}{0.3,0.3,0.3}
\definecolor{lightgray}{rgb}{0.7,0.7,0.7}
\definecolor{darkblue}{rgb}{0.2,0.2,1.0}
\definecolor{darkgreen}{rgb}{0.0,0.5,0.3}

\renewcommand{\labelitemi}{\textcolor{bluegray}\textbullet}
\renewcommand{\labelitemii}{\textcolor{bluegray}{--}}

\setlength{\labelsep}{0.5em}


% see documentation for a0poster class for the size options here
\let\Textsize\normalsize
%\def\Head#1{\noindent\hbox to \hsize{\hfil{\LARGE\color{bluegray} #1}}\bigskip}
\def\Head#1{\noindent{\LARGE\color{bluegray} #1}\bigskip}
\def\LHead#1{\noindent{\LARGE\color{bluegray} #1}\bigskip}
\def\Subhead#1{\noindent{\large\color{bluegray} #1}\bigskip}
\def\Title#1{\noindent{\VeryHuge\color{Red} #1}}

\usepackage{multicol}
\setlength{\columnsep}{1cm}

% Set up the grid
%
% Note that [40mm,40mm] is the margin round the edge of the page --
% it is _not_ the grid size. That is always defined as 
% PAGE_WIDTH/HGRID and PAGE_HEIGHT/VGRID. In this case we use
% 23 x 12. This gives us three columns of width 7 boxes, with a gap of
% width 1 in between them. 12 vertical boxes is a good number to work
% with.
%
% Note however that texblocks can be positioned fractionally as well,
% so really any convenient grid size can be used.
%
\TPGrid[40mm,40mm]{23}{12}      % 3 cols of width 7, plus 2 gaps width 1

\parindent=0pt
\parskip=0.2\baselineskip

\begin{document}

% Understanding textblocks is the key to being able to do a poster in
% LaTeX. In
%
%    \begin{textblock}{wid}(x,y)
%    ...
%    \end{textblock}
%
% the first argument gives the block width in units of the grid
% cells specified above in \TPGrid; the second gives the (x,y)
% position on the grid, with the y axis pointing down.

% You will have to do a lot of previewing to get everything in the 
% right place.

% This gives good title positioning for a portrait poster.
% Watch out for hyphenation in titles - LaTeX will do it
% but it looks awful.
\begin{textblock}{23}(0,0)
\Title{Diabetes Prediction with Incomplete Patient Data}
\end{textblock}

\begin{textblock}{23}(0,0.6)
{
\LARGE
Hao Yi Ong,
Dennis Wang,
Xiao Song Mu
}

{
\Large
\color{bluegray}
\emph{CS 221 Artificial Intelligence: Principles and Techniques Class Project}
}
\end{textblock}


% Uni logo in the top right corner. A&A in the bottom left. Gives a
% good visual balance, but you may want to change this depending upon
% the graphics that are in your poster.
%\begin{textblock}{2}(0,10)
%Your logo here
%%\includegraphics{/usr/local/share/images/AandA.epsf}
%\end{textblock}

% \begin{textblock}{3}(21.0,0)
% % Another logo here
% \resizebox{1.95\TPHorizModule}{!}{\includegraphics{uni-logo.png}}
% \end{textblock}


\begin{textblock}{7.0}(0,1.5)

\hrule\medskip
\Head{Introduction}

\begin{itemize}
  
  \item Given a set of electronic health records, we want to have a smart predictor that prompts high-risk patients to obtain Type II Diabetes testing

\end{itemize}

\begin{figure}[!h]
  \centering
  % \includegraphics[width=9in]{intro-fig.pdf}
  \label{fig1}
\end{figure}

\begin{multicols}{2}
  
  \begin{itemize}
    \item Original Kaggle challenge:
    \begin{itemize}
      \item Patients all have a standard database and a full medical record
      \item I.e., exact same tests taken, same variables recorded, etc.
    \end{itemize}

    \item In practice, however:
    \begin{itemize}
      \item Not everyone has taken the same tests and gotten regular checkups
      \item Database inconsistencies or errors in inputting data may exist
    \end{itemize}
  \end{itemize}
\end{multicols}

\begin{itemize}

  \item Predictor must be able to accurately classify based on incomplete or erroneous medical records to be useful

\end{itemize}

\medskip
\hrule\medskip
\Head{Diabetes Prediction}

\begin{itemize}

  \item Given
  \begin{itemize}
    \item \textbf{Training set} containing standardized patient medical records
    \item \textbf{Testing set} containing patient medical records with missing information and erroneously recorded data
  \end{itemize}

  \item Output
  \begin{itemize}
    \item \textbf{Bayesian network structure} encoding the conditional dependencies between medical record variables
    \item \textbf{Bayesian network parameters} encoding the conditional probabilities for each variable
    \item \textbf{Probabilistic inference} on the learned Bayesian network (BN) for classification
  \end{itemize}

  \item To minimize the error rate, including false positives and false negatives, on classifying whether a patient has Type II Diabetes

\end{itemize}

\medskip
\hrule\medskip
\Head{Evaluation Criteria}

\begin{figure}[!h]
  \centering
  % \includegraphics[width=0.8\textwidth]{eval-fig.pdf}
  \label{fig2}
\end{figure}

\begin{multicols}{2}
  \begin{itemize}
    
    \item \textbf{Baseline: logistic regression}
    \begin{itemize}
      \item Basic features: age, BMI, ...
      \item 10\% hold-out cross validation
      \item False positive rate of 0.7\%, false negative rate of 15.3\%, and an error rate of 16\% (84\% accuracy)
    \end{itemize}

    \item \textbf{Oracle}
    \begin{itemize}
      \item Ideal oracle: experienced physicians (impractical)
      \item Use as surrogate diabetes tests vetted by HHS (HBA1c, FPG, and OGTT; 85--95\% accuracy)
    \end{itemize}

  \end{itemize}
\end{multicols}

\end{textblock}

\begin{textblock}{7.0}(8,1.5)

\hrule\medskip
\Head{Structure Learning as a Search Problem}

\begin{itemize}
  
  \item \textbf{NP-hard search problem} Finding the best set of edges is hard as the number of BNs (DAGs) grows superexponentially with the number of nodes

  \item \textbf{Bayesian score} To evaluate the BN, we use the Bayesian score with a uniform Dirichlet prior over structures, which optimally balances the complexity of the BN structure with the available data (Koller and Friedman, 2009)

  \item \textbf{Feature selection} We selected 27 discretized features (nodes) for the 10,000 samples according to ICD9 disease groupings to limit the search space

  \item \textbf{Tabu search} To find the optimal structure, we use a hill-climbing algorithm that maximizes the ``fitness'' of the BN based on the Bayesian score

  By maintaining a tabu list of recent operators we applied (e.g., adding an edge to the existing BN) and not considering operators that reverse the effect of recently applied operators, the heuristic avoids getting stuck at local optima

  \begin{addmargin}[1em]{2em}% 1em left, 2em right
    \begin{leftbar}
      \begin{tabbing}
        {\bf given} training set \(\mathcal{D}\), structure prior \(P(\mathcal{G})\), node set \(\mathcal{N}\), tabu list size \(L\) \\*[\smallskipamount]
        {\bf initialize} random BN \(\mathcal{G}\), tabu list \(\mathcal{T}\), valid operations \(\mathcal{O}\) \\*[\smallskipamount]
        {\bf repeat} \\
        \qquad \= 1.\ if \(\left|\mathcal{O}\right|\) too large, generate random subset of valid operations \(\hat{\mathcal{O}}\subset\mathcal{O}\) \\
        \> 2.\ find best operation: \(\text{Op} := \argmax_{\text{Op}\in\hat{\mathcal{O}}\backslash \mathcal{T}}\text{BayesScore}(\text{Op}(\mathcal{G}))\) \\
        \> 3.\ set \(\mathcal{G}:=\text{Op}(\mathcal{G})\) and \(\mathcal{T}:=\mathcal{T}\cup\left\{\text{reverse}(\text{Op})\right\}\) \\
        \> 4.\ remove operation added \(L\) iterations ago to \(\mathcal{T}\) from it \\
        {\bf until} \(\text{BayesScore}(\mathcal{G})\) converges \\*[\smallskipamount]
        {\bf return} \(\mathcal{G}\)
      \end{tabbing}
    \end{leftbar}
  \end{addmargin}
  
  \item \textbf{Resulting structures} Produced 20 structures; avg 31 min with Julia implementation on an Intel Xeon E5-2650 processor (2.60 GHz), 32 GB RAM

\end{itemize}

\medskip
\hrule\medskip
\Head{Parameter Learning}

\begin{itemize}
  
  \item \textbf{MLE with Laplace smoothing} We use MLE with Laplace smoothing to learn the conditional probabilities associated with each node (9,000 samples)

  \item \textbf{Limited dataset} Given dataset is small in comparison to number of possible variable values, which motivates our use of Laplace smoothing

  \item \textbf{Relational database} We store our data in a relational database and optimize data retrieval by applying indices over sets of variable names

\end{itemize}

\medskip
\hrule\medskip
\Head{Approximate Probabilistic Inference}

\begin{itemize}

  \item \textbf{Classification using BN} Assign label to whichever inferred probability is higher; probabilities are over the missing variables and observed ones

  \item \textbf{NP-hard} Given our large BN structure and variable domain sizes, exact inference is infeasible (exponential time complexity in the worst case)

  \item \textbf{Gibbs sampling} We use a Markov Chain Monte Carlo technique where, in the limit, samples are drawn exactly from the joint distribution over the unknown variables given the observed variables

  Since samples are not independent, we discard the first 100 samples (burn-in period) and keep only every 10th sample (thinning)

\end{itemize}

\end{textblock}

  
\begin{textblock}{7.0}(16,1.5)

\hrule\medskip
\Head{Results and Analysis}

\begin{itemize}
  
  \item \textbf{Best structure} We picked the top 8 structures based on the Bayesian score, ran parameter learning on them, and tested their prediction accuracies based on classifying test sets using inference

\end{itemize}

\end{textblock}

\begin{textblock}{7.0}(16,2.75)

\begin{figure}[!h]
  \centering
  \hbox{
    \hspace{-2em}
    % \includegraphics[width=1.1\textwidth]{bn-struct.pdf}
  }
  \label{fig3}
\end{figure}

\end{textblock}

\begin{textblock}{7.0}(16,5.25)

\begin{itemize}

  \item \textbf{Error rates} Varying the number of missing variables, the error rates were 15--19\% on 1,000 test samples, which are competitive with the 16\% baseline

  \item \textbf{Runtime} Depending on the number of missing variables, a single prediction can take up to 30 seconds---much more expensive than our baseline

\end{itemize}

\end{textblock}

\begin{textblock}{7.0}(16,6.2)

\begin{figure}[!h]
  \centering
  % \includegraphics[trim=3pt 3pt 3pt 10pt, clip, width=0.8\textwidth]{../src/Excel/BayesStructureChart.pdf}
  \label{fig3}
\end{figure}

\begin{table}[htbp!]
  \begin{minipage}{\textwidth}
    \centering
    \caption*{Summary: Classification results with varying number of missing features}
    \begin{tabular}{|c||c|c|c|c|}
      \hline
      \#missing & Error rate & False positive & False negative & Pred time (sec) \\ \hline \hline
      3 & 0.1662 & 0.0363 & 0.1299 & 3.754 \\ \hline
      5 & 0.1511 & 0.0302 & 0.1208 & 6.437 \\ \hline
      8 & 0.1339 & 0.0272 & 0.1067 & 9.672 \\ \hline
      10 & 0.1299 & 0.0252 & 0.1047 & 11.859 \\ \hline
    \end{tabular}
    \label{table:summary}
  \end{minipage} 
\end{table}

\medskip
\hrule\medskip
\Head{Conclusion and Extensions}

\begin{itemize}
  
  \item Our model is suitable for diabetes prediction on existing databases (even with incomplete records) to detect patients at risk for diabetes
  
  \item Incorporate expert knowledge in feature selection and deciding the structure
  
  \item Consider different inference methods for runtime and accuracy improvement

\end{itemize}

\medskip
\hrule\medskip

*We thank the course instructor Prof. Percy Liang, our mentor Billy Jun, and the instructor team

\end{textblock}

\end{document}
